{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e69413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import math\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy import signal\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cb9269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatSLE(WDEjson):\n",
    "    '''\n",
    "    Dobbiamo raggiungere questo formato:\n",
    "    {\n",
    "        \"target\": stride_lenght of stride index_inizio-index-fine,\n",
    "        \"Acc_X\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Acc_Y\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Acc_Z\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_X\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_Y\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_Z\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"SensorTimestamp\": [list of timestamp of stride index_inizio-index-fine]\n",
    "    }\n",
    "    Data_lable=[\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\"SensorTimestamp\"]    \n",
    "    '''\n",
    "    SLEjson={}\n",
    "    SLEjson[\"target\"]=WDEjson[\"stride_plength\"]\n",
    "    SLEjson[\"Acc_X\"]=WDEjson[\"sensors\"][\"acc\"][\"acc_x\"]\n",
    "    SLEjson[\"Acc_Y\"]=WDEjson[\"sensors\"][\"acc\"][\"acc_y\"]\n",
    "    SLEjson[\"Acc_Z\"]=WDEjson[\"sensors\"][\"acc\"][\"acc_z\"]\n",
    "    SLEjson[\"Gyr_X\"]=WDEjson[\"sensors\"][\"gyro\"][\"gyr_x\"]\n",
    "    SLEjson[\"Gyr_Y\"]=WDEjson[\"sensors\"][\"gyro\"][\"gyr_y\"]\n",
    "    SLEjson[\"Gyr_Z\"]=WDEjson[\"sensors\"][\"gyro\"][\"gyr_z\"]\n",
    "    SLEjson[\"SensorTimestamp\"]=WDEjson[\"sensors\"][\"timestamp\"]\n",
    "    return SLEjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3db6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importWDE(data_path: str=\"C:/Users/aliba/OneDrive/Desktop/UNIVERSITA/TESI/DATASET/WalkingDistanceEstimation-master/dataset/\"):\n",
    "    filename_list_WDE=[\"PDR_Raw_2019-03-20-09-10-12\",\"PDR_Raw_2019-03-20-09-21-02\",\"PDR_Raw_2019-03-20-09-29-55\",\"PDR_Raw_2019-03-21-08-32-39\",\"PDR_Raw_2019-03-21-09-07-51\",\"PDR_Raw_2019-03-21-11-57-56\",\"PDR_Raw_2019-03-24-11-12-21\",\"PDR_Raw_2019-03-28-11-50-11\",\"PDR_Raw_2019-03-29-07-37-22\",\"PDR_Raw_2019-03-29-08-30-54\",\"PDR_Raw_2019-03-30-11-29-16\",\"PDR_Raw_2019-03-31-01-23-59\",\"PDR_Raw_2019-03-31-10-04-54\",\"PDR_Raw_2019-03-31-10-33-25\",\"PDR_Raw_2019-03-31-12-03-05\",\"PDR_Raw_2019-03-31-12-29-51\",\"PDR_Raw_2019-04-01-10-45-07\",\"PDR_Raw_2019-04-02-08-44-50\"]\n",
    "    classi=['armhand', 'pocket', 'calling', 'swing', 'handheld']\n",
    "\n",
    "    DATASET={'armhand':[], 'pocket':[], 'calling':[], 'swing':[], 'handheld':[]}#dict divided by mode, values are stride_list wich are list of dict (stride)\n",
    "    stop_lists={'armhand':[0], 'pocket':[0], 'calling':[0], 'swing':[0], 'handheld':[0]}#dict divided by mode, values are index to split for subject\n",
    "    for filename in filename_list_WDE:\n",
    "        count_dict={'armhand':0, 'pocket':0, 'calling':0, 'swing':0, 'handheld':0}\n",
    "        count_elim={'armhand':0, 'pocket':0, 'calling':0, 'swing':0, 'handheld':0}\n",
    "        with open(data_path+filename+\".txt\",'r') as f:\n",
    "            for line in f:\n",
    "                stride=json.loads(line)\n",
    "            \n",
    "                #gli outliers non li carico direttamente\n",
    "                if is_in_range(stride[\"stride_plength\"],[1,2]) and is_in_range(len(stride[\"sensors\"][\"timestamp\"]),[50,300]) :\n",
    "                    if filename!=\"PDR_Raw_2019-03-31-10-33-25\" or stride[\"mode\"]!=\"calling\":\n",
    "                        DATASET[stride[\"mode\"]].append(formatSLE(stride))\n",
    "                        count_dict[stride[\"mode\"]]+=1\n",
    "                else:\n",
    "                    count_elim[stride[\"mode\"]]+=1\n",
    "                \n",
    "        print(filename,count_dict)\n",
    "        for k,v in count_dict.items():\n",
    "            if v!=0:\n",
    "                stop_lists[k].append(stop_lists[k][-1]+v)\n",
    "        print(\"Outliers eliminati\\t\",count_elim,\"\\n\")\n",
    "\n",
    "    print(f\"\\nIn totale=>\",end=\" \")\n",
    "    s=0\n",
    "    for k,v in DATASET.items():\n",
    "        print(f\"{k}:{len(v)}\",end=\", \")\n",
    "        s+=len(v)\n",
    "    print(f\"-->{s} stride\")\n",
    "    \n",
    "    return DATASET, stop_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5f2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtWDE(DATASET):\n",
    "    print(\"Filtering:\",end=\"\")\n",
    "    #buttfilter ogni lista di stride\n",
    "    for mode,stride_list in DATASET.items():\n",
    "        clean_stride_list=[]\n",
    "        for i in range(len(stride_list)):\n",
    "            if i==0 or i==len(stride_list)-1:\n",
    "                print(\"#\",end=\"\")            \n",
    "            \n",
    "            clean_stride_list.append(SLE_buttfilter(stride_list[i]))\n",
    "        #print(type(clean_stride_list),type(clean_stride_list[0]))\n",
    "        DATASET[mode]=clean_stride_list\n",
    "    print(\"\\nDone!\\n\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29769062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ext_WDE(DATASET):\n",
    "    c=0\n",
    "    #estraiamo le feature di ogni stride\n",
    "    Feature_DATASET={'armhand':{'feature':[],'target':[]}, 'pocket':{'feature':[],'target':[]}, 'calling':{'feature':[],'target':[]}, 'swing':{'feature':[],'target':[]}, 'handheld':{'feature':[],'target':[]}}\n",
    "    for mode,stride_list in DATASET.items():\n",
    "        print(f\"\\nExtracting {mode}:\",end=\"\")\n",
    "        c=0\n",
    "        for stride in stride_list:\n",
    "            c+=1\n",
    "            Feature_DATASET[mode]['feature'].append(feature_extraction(stride))\n",
    "            Feature_DATASET[mode]['target'].append(stride['target'])\n",
    "            d=np.linspace(0,len(stride_list)-1,num=10,endpoint=True,dtype= int).tolist()\n",
    "            if c in d:\n",
    "                print(\"#\",end=\"\")\n",
    "    return Feature_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5d92ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeeqWDE(Feature_DATASET,n_train: int=500,n_test: int=50):\n",
    "    #facciamolo equilibrato\n",
    "    #train i primi 500 elem di ogni modalità, test gli ultimi 50 di ogni modalità\n",
    "    DATASET_train={'armhand':{'feature':[],'target':[]}, 'pocket':{'feature':[],'target':[]}, 'calling':{'feature':[],'target':[]}, 'swing':{'feature':[],'target':[]}, 'handheld':{'feature':[],'target':[]}}\n",
    "    DATASET_test={'armhand':{'feature':[],'target':[]}, 'pocket':{'feature':[],'target':[]}, 'calling':{'feature':[],'target':[]}, 'swing':{'feature':[],'target':[]}, 'handheld':{'feature':[],'target':[]}}\n",
    "\n",
    "    for mode, diz in Feature_DATASET.items():\n",
    "    \n",
    "        DATASET_train[mode]['feature']=diz['feature'][0:n_train] \n",
    "        DATASET_train[mode]['target'] =diz['target'][0:n_train]\n",
    "\n",
    "        DATASET_test[mode]['feature']=diz['feature'][-n_test:] \n",
    "        DATASET_test[mode]['target'] =diz['target'][-n_test:]\n",
    "    return DATASET_train, DATASET_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b8fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trueeqWDE(Feature_DATASET,stop_lists,n_train: int=500,n_test: int=50):\n",
    "    #facciamolo equilibrato\n",
    "    #train i primi 500 elem di ogni modalità, test gli ultimi 50 di ogni modalità\n",
    "    DATASET_train={'armhand':{'feature':[],'target':[]}, 'pocket':{'feature':[],'target':[]}, 'calling':{'feature':[],'target':[]}, 'swing':{'feature':[],'target':[]}, 'handheld':{'feature':[],'target':[]}}\n",
    "    DATASET_test={'armhand':{'feature':[],'target':[]}, 'pocket':{'feature':[],'target':[]}, 'calling':{'feature':[],'target':[]}, 'swing':{'feature':[],'target':[]}, 'handheld':{'feature':[],'target':[]}}\n",
    "\n",
    "    for mode, list_stop in stop_lists.items():\n",
    "        \n",
    "        #lista della lunghezza dei vari file in ordine\n",
    "        l_s=[list_stop[i+1]-list_stop[i] for i in range(len(list_stop)-1)]\n",
    "        \n",
    "        #prendi il più piccolo e lo usi per il test\n",
    "        #index of shortest file\n",
    "        if min(l_s)<n_test:\n",
    "            del l_s[l_s.index(min(l_s))]\n",
    "\n",
    "        ind_ts=l_s.index(min(l_s))\n",
    "\n",
    "        #print(\"test\",ind_ts,list_stop[ind_ts+1]-n_test,list_stop[ind_ts+1])\n",
    "        DATASET_test[mode]['feature']=Feature_DATASET[mode]['feature'][list_stop[ind_ts+1]-n_test:list_stop[ind_ts+1]] \n",
    "        DATASET_test[mode]['target'] =Feature_DATASET[mode]['target'][list_stop[ind_ts+1]-n_test:list_stop[ind_ts+1]]\n",
    "        \n",
    "        #i più grandi dividi il n_train per il numero \n",
    "        split_ntrain=int(n_train/(len(l_s)-1))\n",
    "        \n",
    "        n_file=0\n",
    "        flag=0\n",
    "        while len(DATASET_train[mode]['feature'])<n_train:\n",
    "            \n",
    "            count=flag\n",
    "            if n_file==ind_ts:\n",
    "                n_file+=1\n",
    "                \n",
    "            while (count<split_ntrain+flag and count!=l_s[n_file] and len(DATASET_train[mode]['feature'])!=n_train):\n",
    "                \n",
    "                #print(mode,count,n_file,count+list_stop[n_file])\n",
    "                DATASET_train[mode]['feature'].append(Feature_DATASET[mode]['feature'][count+list_stop[n_file]]) \n",
    "                DATASET_train[mode]['target'].append(Feature_DATASET[mode]['target'][count+list_stop[n_file]])\n",
    "                \n",
    "                count+=1\n",
    "            \n",
    "            n_file+=1\n",
    "            n_file=n_file%(len(l_s))\n",
    "            if n_file==0:\n",
    "                n_file=l_s.index(max(l_s))\n",
    "                flag=split_ntrain\n",
    "\n",
    "        \n",
    "    return DATASET_train, DATASET_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbafa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_regr_dataset(DATASET_train,DATASET_test):\n",
    "    #funizone make_regr che unisce tutti i feature e tutti i target in una sola lista per avere F_x ed F_y della regression\n",
    "    ####REGRESSION DATASET FOR INDISTINGUISHED MODE\n",
    "    Regr_Dataset_train={\"feature\":[],\"target\":[]}\n",
    "    Regr_Dataset_test={\"feature\":[],\"target\":[]}\n",
    "\n",
    "    for k in DATASET_train.keys():\n",
    "    \n",
    "        Regr_Dataset_train[\"feature\"]+=DATASET_train[k][\"feature\"]\n",
    "        Regr_Dataset_train[\"target\"]+=DATASET_train[k][\"target\"]\n",
    "    \n",
    "        Regr_Dataset_test[\"feature\"]+=DATASET_test[k][\"feature\"]\n",
    "        Regr_Dataset_test[\"target\"]+=DATASET_test[k][\"target\"]\n",
    "    \n",
    "    return Regr_Dataset_train, Regr_Dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e5f281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_regr_dataset(Feature_DATASET,label: str=\"handheld\"):\n",
    "    #funzione che seleziona solo una modalità\n",
    "\n",
    "    #NON SERVE, PRENDI IL PUNTO DI SVILUPPO CHE TI INTERESSA E FAI DATASET[modalità] e te lo salvi dove cazzo vuoi\n",
    "    #vediamo che l'ultimo file ha 664 handheld, quindi lo teniamo comme test isolando gli ultimi 664 stride\n",
    "    ####REGRESSION DATA FOR SINGLE MODE\n",
    "    Sreg_Dataset_train={\"feature\":[],\"target\":[]}\n",
    "    Sreg_Dataset_test={\"feature\":[],\"target\":[]}\n",
    "\n",
    "    if label==\"handheld\":\n",
    "        ind=664#numero di elementi dell'ULTIMO soggetto\n",
    "    elif label==\"pocket\":\n",
    "        ind=180#numero di elementi dell'ULTIMO soggetto\n",
    "    elif label==\"calling\":\n",
    "        ind=123#numero di elementi dell'ULTIMO soggetto\n",
    "    elif label==\"swing\":\n",
    "        ind=232#numero di elementi dell'ULTIMO soggetto\n",
    "    elif label==\"armhand\":\n",
    "        ind=70#numero di elementi dell'ULTIMO soggetto\n",
    "        \n",
    "    Sreg_Dataset_train['feature']=Feature_DATASET[label]['feature'][0:-ind]\n",
    "    Sreg_Dataset_train['target']=Feature_DATASET[label]['target'][0:-ind]\n",
    "\n",
    "    Sreg_Dataset_test['feature']=Feature_DATASET[label]['feature'][-ind:]\n",
    "    Sreg_Dataset_test['target']=Feature_DATASET[label]['target'][-ind:]\n",
    "    \n",
    "    return Sreg_Dataset_train, Sreg_Dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff0123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_dataset(DATASET_train,DATASET_test):\n",
    "    #funzione per il dataset della classificazione\n",
    "    #prendi il dataset equilibrato e generi i target della classificazione Mode_list\n",
    "    #DATASET_train e DATASET_test\n",
    "\n",
    "    classification_feature_train=[]\n",
    "    classification_target_train=[]\n",
    "\n",
    "    classification_feature_test=[]\n",
    "    classification_target_test=[]\n",
    "\n",
    "    for mode in DATASET_train.keys():\n",
    "        classification_feature_train+=DATASET_train[mode]['feature']\n",
    "        n=len(DATASET_train[mode]['feature'])\n",
    "        mode_list=[mode]*n\n",
    "        classification_target_train+=mode_list\n",
    "    \n",
    "        classification_feature_test+=DATASET_test[mode]['feature']\n",
    "        n=len(DATASET_test[mode]['feature'])\n",
    "        mode_list=[mode]*n\n",
    "        classification_target_test+=mode_list\n",
    "        \n",
    "    return classification_feature_train,classification_target_train,classification_feature_test,classification_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c6d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_range(num,rng):\n",
    "    return num <= rng[1] and num >= rng[0]\n",
    "def del_outliers(Stride_list,mode_list: list=[] ,target_rng: list=[1,2],time_rng: list=[80,300]):\n",
    "    \n",
    "    clearedS=[]\n",
    "    clearedM=[]\n",
    "    for i in range(len(Stride_list)):\n",
    "        diz=Stride_list[i]\n",
    "        #don't save outliers\n",
    "        if is_in_range(diz[\"target\"],target_rng) and is_in_range(len(diz[\"Acc_X\"]),time_rng) :#if true is in range\n",
    "            clearedS.append(diz)\n",
    "            if len(mode_list)!=0:\n",
    "                clearedM.append(mode_list[i])\n",
    "    \n",
    "    return clearedS, clearedM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bcb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_list_range(lista,n_elem,randomize: bool= False):\n",
    "    '''\n",
    "    Return tuple: a rand_list of  randomize (random) n_elem elements of lista , left_list of leftovers\n",
    "    if n_elem==-1 return lista,[] return full list with no leftovers\n",
    "    Return [],lista if an error occurs\n",
    "    '''\n",
    "    #se la ista ha meno elementi di quelli che vogliamo selezionare allora ritorniamo una lista vuota\n",
    "    if len(lista) < n_elem or n_elem < -1:\n",
    "        return [], lista\n",
    "    \n",
    "    if n_elem == -1:\n",
    "        return lista, []\n",
    "    \n",
    "    #se non vogliamo randomizzare prendiamo i primi n_elem elementi\n",
    "    if  not randomize:\n",
    "        return lista[0:n_elem], lista[n_elem:]\n",
    "\n",
    "    #altrimenti prendiamo n_elem casuali dalla lista\n",
    "    random_index_set=set([])\n",
    "    while len(random_index_set) != n_elem:\n",
    "        n=random.randint(0,len(lista)-1)\n",
    "        random_index_set.add(n)\n",
    "    \n",
    "    rand_list=[]\n",
    "    left_list=[]\n",
    "    for i in range(len(lista)):  \n",
    "        if i in random_index_set:\n",
    "            rand_list.append(lista[i])\n",
    "        else:\n",
    "            left_list.append(lista[i])\n",
    "        \n",
    "    return rand_list, left_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df68f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eqdataset(full_x,full_y,list_stop,n_elem,randomize: bool=False):\n",
    "    new_x=[]\n",
    "    new_y=[]\n",
    "    n_s=0\n",
    "    \n",
    "    start=0\n",
    "    for stop in list_stop:\n",
    "        subject_x=full_x[start:stop]\n",
    "        subject_y=full_y[start:stop]\n",
    "        start=stop\n",
    "        \n",
    "        x, _ = random_list_range(subject_x,n_elem,randomize= randomize)\n",
    "        y, _ = random_list_range(subject_y,n_elem,randomize= randomize)\n",
    "        \n",
    "        if (len(x) == len(y)) and (len(x)>0):\n",
    "            new_x+=x\n",
    "            new_y+=y\n",
    "            n_s+=1\n",
    "        \n",
    "    print(f\"####\\nDataset equilibrato, scelti {n_elem} stride per ogni soggetto.\\nRisultato: {len(new_x)} stride per {n_s} subject.\\n#####\")\n",
    "    return new_x, new_y\n",
    "def filenames(filename_list,n_elem,stop_list):\n",
    "    filt_filename=[]\n",
    "    c=0\n",
    "    for i in range(len(stop_list)):\n",
    "        if (stop_list[i]-c)>=n_elem:\n",
    "                filt_filename.append(filename_list[i])\n",
    "        c=stop_list[i]\n",
    "        \n",
    "    return filt_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c67503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_plot_range(X,Y):\n",
    "    return [min([min(X),min(Y)]),max([max(X),max(Y)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f502bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ist_stride_lenght(vector_of_stride_lenght):\n",
    "    print(\"Registered Stride lenght: Min: \",min(vector_of_stride_lenght),\", Max: \", max(vector_of_stride_lenght))\n",
    "    #bin width of 1cm\n",
    "    bw=0.01\n",
    "    plt.hist(vector_of_stride_lenght,bins=np.arange(min(vector_of_stride_lenght), max(vector_of_stride_lenght) + bw, bw))\n",
    "    plt.xlim([min(vector_of_stride_lenght), max(vector_of_stride_lenght)])\n",
    "    media=np.mean(np.array(vector_of_stride_lenght,dtype=float))\n",
    "    st_d=np.std(np.array(vector_of_stride_lenght,dtype=float))\n",
    "    print(f\"Media= {str(media)}; Deviazione standard= {str(st_d)}.\")\n",
    "    plt.xlabel(\"Lunghezza dello stride/ mt\")\n",
    "    plt.ylabel(\"Numero di sample/ n\")\n",
    "    plt.show()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05dc6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSLE(dati_json,n_sample: int = -1):\n",
    "    if n_sample > 0 :\n",
    "        n_sample+=50\n",
    "        \n",
    "    time_stamp=[x-dati_json[\"SensorTimestamp\"][0] for x in dati_json[\"SensorTimestamp\"]]\n",
    "    \n",
    "    for k,v in dati_json.items():\n",
    "        if (type(v) is list) and (k != \"flag\") and (k != \"SensorTimestamp\") and (k != \"stride_number\") and (k != \"stride_length\") and (k != \"walking_distance\"):\n",
    "            plt.plot(time_stamp[50:n_sample],v[50:n_sample],color='r')\n",
    "            plt.ylabel(k)\n",
    "            plt.xlabel(\"Time \\ ms\")\n",
    "            plt.ylim([min(v[50:n_sample]),max(v[50:n_sample])])\n",
    "            plt.title(\"Grafico del \"+ k)\n",
    "            plt.figure()\n",
    "            plt.show()\n",
    "        elif (k == \"stride_length\"):\n",
    "            plt.figure()\n",
    "            ist_stride_lenght(v)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "458d2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_magnitude(dati_x,dati_y,dati_z):\n",
    "    '''\n",
    "    calcoliamo il vettore Magnitude da un vettore 3d\n",
    "    '''\n",
    "    M=[]\n",
    "    for i in range(len(dati_x)):\n",
    "        \n",
    "        M.append(math.sqrt((dati_x[i]*dati_x[i])+(dati_y[i]*dati_y[i])+(dati_z[i]*dati_z[i])))\n",
    "\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36b4a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(y_true,y_pred):\n",
    "    '''\n",
    "    Input= (N-1darray) true_value, (N-1darray) predicted value\n",
    "    Output= error_rate % between the input arrays \n",
    "    '''\n",
    "    count=0\n",
    "    for i in range(len(y_true)):\n",
    "        count+=(abs(y_pred[i]-y_true[i])/y_true[i])\n",
    "\n",
    "    return (count/len(y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a303f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(data_ing):#calcoliamo ogni feature per ogni singolo stream ed anche la magnitude del sensore quindi f di A_x,y,z,||A||,G_x,y,z,||G||\n",
    "    '''\n",
    "    data_ing= json che rappresenta uno stride.\n",
    "    \n",
    "    f_Ax,..,f_Gm = < f_mean, f_std, f_ske, f_kurt, f_iqr, f_Ma, f_zc, f_A1, f_F1, f_A2, f_F2 > calcolato per < Acc_X, Acc_Y, Acc_Z, M_Acc, Gyr_X, Gyr_Y, Gyr_Z, M_Gyr > \n",
    "    \n",
    "    OUTPUT=> ( 1d list) feature_vector =[f_Ax,f_Ay,f_Az,f_Gx,f_Gy,f_Gz,f_Am,f_Gm,f_CC,f_wei,f_kim,f_scar]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    data_json=copy.deepcopy(data_ing)#per non modificare data_ing\n",
    "\n",
    "    data_json[\"M_Acc\"]=f_magnitude(data_json[\"Acc_X\"],data_json[\"Acc_Y\"],data_json[\"Acc_Z\"])\n",
    "    data_json[\"M_Gyr\"]=f_magnitude(data_json[\"Gyr_X\"],data_json[\"Gyr_Y\"],data_json[\"Gyr_Z\"])\n",
    "    \n",
    "    #Data_lable=[\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\"M_Acc\",\"M_Gyr\"]\n",
    "    \n",
    "    feature_vector=[]\n",
    "    for lable, stream in data_json.items():#per ogni stream di misuraizoni\n",
    "        if (type(stream) is list) and (lable != \"SensorTimestamp\"):\n",
    "            #FEATURE STATISTICHE-----------------------------------\n",
    "            f_mean=sum(stream)/len(stream)\n",
    "            f_std=np.std(np.array(stream))  \n",
    "            f_skew=ss.skew(stream)\n",
    "            f_kurt=ss.kurtosis(stream)\n",
    "\n",
    "            #calculate interquartile range \n",
    "            q3, q1 = np.percentile(np.array(stream), [75 ,25])\n",
    "            f_iqr = q3 - q1\n",
    "\n",
    "            #magnitude area= somma dei valori assoluti di un segnale\n",
    "            stream_abs=[abs(x) for x in stream]\n",
    "            f_Ma=sum(stream_abs)\n",
    "\n",
    "            #FEATURE TEMPORALI-------------------------------------\n",
    "\n",
    "            limite=max(stream)*95/100#la threshold per rilevare il picco mettiamo che sia il 95% del valore massimo\n",
    "            f_peak=0\n",
    "            #counting crossing zero srutto lo stesso for\n",
    "            media_stream=np.mean(stream)\n",
    "            f_zc=0\n",
    "            for i in range(len(stream)):\n",
    "                #peak\n",
    "                if stream[i]>=limite:\n",
    "                    f_peak+=1\n",
    "                #zero crossing\n",
    "                if i!=0:\n",
    "                    if ( ((stream[i]-media_stream)*(stream[i-1]-media_stream)) < 0):\n",
    "                        f_zc+=1\n",
    "\n",
    "\n",
    "            #FEATURE FREQUENZIALI-------------------------------------\n",
    "            #FFT of the stream without mean to find amplitude\n",
    "            amplitude=np.abs(fft((stream-media_stream)))\n",
    "            #troviamo le frequenze della serie temporale\n",
    "            sample_rate= 100#frequenza di cmapionamento Hz\n",
    "            N=len(amplitude)#number of sample n\n",
    "\n",
    "            frequences= fftfreq(N, d= (1 / sample_rate))\n",
    "            #take only positives\n",
    "            frequences=[x for x in frequences if x >=0]\n",
    "            #take amplitude\n",
    "            amplitude=amplitude[0:len(frequences)]\n",
    "            f_A1=np.amax(amplitude)\n",
    "            f_F1=frequences[int((np.where(amplitude==f_A1)[0][0]))]\n",
    "            amplitude[int((np.where(amplitude==f_A1)[0][0]))]=0\n",
    "            f_A2=np.amax(amplitude)\n",
    "            f_F2=frequences[int((np.where(amplitude==f_A2)[0][0]))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #creiamo il vettore feature_vector=[mean,std,skew,kurt,iqr,Ma,peak,zc |per ogni Acc_x,Acc_y...]\n",
    "            feature_vector.append(f_mean)\n",
    "            feature_vector.append(f_std)\n",
    "            feature_vector.append(f_skew)\n",
    "            feature_vector.append(f_kurt)\n",
    "            feature_vector.append(f_iqr)\n",
    "            feature_vector.append(f_Ma)\n",
    "            #feature_vector.append(f_peak) #Feature non accurata\n",
    "            feature_vector.append(f_zc)\n",
    "            feature_vector.append(f_A1)\n",
    "            feature_vector.append(f_F1)\n",
    "            feature_vector.append(f_A2)\n",
    "            feature_vector.append(f_F2)\n",
    "\n",
    "    #FEATURE CROSS-SENSORI--------------------------------------\n",
    "    #correlazione tra M_A ed M_G\n",
    "    f_cc_AG = np.corrcoef(np.array(data_json[\"M_Acc\"]),np.array(data_json[\"M_Gyr\"]))[0,1]\n",
    "\n",
    "\n",
    "    \n",
    "    #FEATURE DI ALTO LIVELLO, PDR--------------------------------\n",
    "    #Weinberg\n",
    "    a_max=max(data_json[\"Acc_Z\"])\n",
    "    a_min=min(data_json[\"Acc_Z\"])\n",
    "    f_wei=math.sqrt(math.sqrt(a_max-a_min))\n",
    "\n",
    "\n",
    "    #Kim\n",
    "    M_A_abs=[abs(x) for x in data_json[\"M_Acc\"]]\n",
    "    sum_abs=sum(M_A_abs)\n",
    "    f_kim=(sum_abs/len(data_json[\"M_Acc\"]))**(1/3)\n",
    "\n",
    "\n",
    "    #Scarlett\n",
    "\n",
    "    f_sca=(sum_abs-max(data_json[\"M_Acc\"]))/(max(data_json[\"M_Acc\"])-min(data_json[\"M_Acc\"]))\n",
    "\n",
    "\n",
    "    #aggiungo le feature\n",
    "    feature_vector.append(f_cc_AG)\n",
    "    feature_vector.append(f_wei)\n",
    "    feature_vector.append(f_kim)\n",
    "    feature_vector.append(f_sca)\n",
    "    \n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a16fede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mean(data):\n",
    "    '''\n",
    "    Prende in input un segnale e lo restituisce senza media\n",
    "    '''\n",
    "    data_mean=sum(data)/len(data)\n",
    "    return [x-data_mean for x in data]\n",
    "def SLE_remove_mean(data_json):\n",
    "    Data_lable=[\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\"]\n",
    "  \n",
    "    for x in Data_lable:\n",
    "        data_json[x]=remove_mean(data_json[x])\n",
    "\n",
    "    return 0\n",
    "def SLE_buttfilter(data_json,norder: int=1):\n",
    "    '''\n",
    "    Input = dato_json (con lable \"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\" )\n",
    "    Output= copia del json con le serie corrispondenti ai lable filtrate e castate a liste\n",
    "    '''\n",
    "    Data_lable=[\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\"]#ho eliminato mag_x_y_z\n",
    "\n",
    "    #design filter\n",
    "    sos_filter=signal.butter(norder,3,'low',analog=False,output='sos',fs=100)\n",
    "  \n",
    "    data_filtered=copy.deepcopy(data_json)\n",
    "    SLE_remove_mean(data_filtered)\n",
    "    for x in Data_lable:\n",
    "        #Filt signal\n",
    "        data_filtered[x]=signal.sosfilt(sos_filter,data_json[x]).tolist()\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80057fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividi(dati_json,index_inizio,index_fine):\n",
    "    '''\n",
    "    Input= dati json interi\n",
    "    Output= frammento formattato dei dati iniziali\n",
    "    Formato:\n",
    "    {\n",
    "        \"target\": stride_lenght of stride index_inizio-index-fine,\n",
    "        \"Acc_X\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Acc_Y\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Acc_Z\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_X\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_Y\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"Gyr_Z\": [list of streams of stride index_inizio-index-fine],\n",
    "        \"SensorTimestamp\": [list of timestamp of stride index_inizio-index-fine]\n",
    "    }\n",
    "    '''\n",
    "    Data_lable=[\"Acc_X\",\"Acc_Y\",\"Acc_Z\",\"Gyr_X\",\"Gyr_Y\",\"Gyr_Z\",\"SensorTimestamp\"]\n",
    "\n",
    "    new_dict={}\n",
    "    new_dict[\"target\"]=dati_json[\"stride_length\"][index_fine]\n",
    "    for lable in Data_lable:\n",
    "        new_dict[lable]=dati_json[lable][index_inizio+1:index_fine+1]\n",
    "\n",
    "\n",
    "    return new_dict#ritorna il dizionario diviso per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206f32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(dati_json):\n",
    "    '''\n",
    "    Input= All json\n",
    "    Output= list of dictionary (every dict is a stride)\n",
    "    '''\n",
    "\n",
    "    dati_segmentati=[]\n",
    "    limits=(np.ediff1d(np.array([dati_json[\"stride_number\"]],dtype=float)))\n",
    "    limits=np.append(limits,1.)\n",
    "    inizio=-1\n",
    "    fine=-1\n",
    "    for i in range(len(limits)):\n",
    "        if (limits[i]>0):\n",
    "            #inizio di uno è fine di quello prima (contatto a terra)\n",
    "            inizio=fine\n",
    "            fine=i\n",
    "            dati_segmentati.append(dividi(dati_json,inizio,fine))\n",
    "\n",
    "    if ((len(dati_segmentati))<1):\n",
    "        print(\"Questi dati non possono essere segmentati\")\n",
    "        return {}\n",
    "\n",
    "    #eliminiamo il primo stride che spesso è sfalsato\n",
    "    dati_segmentati.pop(0)\n",
    "\n",
    "    #assicuriamoci che siano tutte liste\n",
    "    for diz in dati_segmentati:\n",
    "        for k,v in diz.items():\n",
    "            if (type(v) is np.ndarray):\n",
    "                diz[k]=v.tolist()\n",
    "    \n",
    "\n",
    "    return dati_segmentati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c9d11",
   "metadata": {},
   "source": [
    "\n",
    "def wdesplit(F_x_WDE,mode_list):\n",
    "    '''\n",
    "    Da F_x_WDE (lista dei vettori di feature) , F_y_WDE (lista dello stride lenght per ogni passo) e mode_list (lista dei lable)\n",
    "    \n",
    "    Restituisce la lista delle feature splittata per lable di feature e target\n",
    "    \n",
    "    '''\n",
    "    classi=['armhand', 'pocket', 'calling', 'swing', 'handheld']\n",
    "    split_dataset=[[],[],[],[],[]]\n",
    "    for i in range(len(mode_list)):    \n",
    "        split_dataset[classi.index(mode_list[i])].append(F_x_WDE[i])\n",
    "        \n",
    "\n",
    "    print(f\"Per {len(split_dataset)} lable:\")\n",
    "    for i in range(len(classi)):\n",
    "        print(f\"\\t{classi[i]}={len(split_dataset[i])}\")\n",
    "        \n",
    "    return split_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6c462",
   "metadata": {},
   "source": [
    "\n",
    "def eqWDE(split_dataset,n_elem: int=600,n_lft: int=85,randomize: bool=False):\n",
    "    '''\n",
    "    Da un dataset splittato per lable lo equilibra restituendo un vettore di vettori di feature equilibrato (n_elem per label)\n",
    "    ed anche un vettore dei label corretti\n",
    "\n",
    "    '''\n",
    "    classi=['armhand', 'pocket', 'calling', 'swing', 'handheld']\n",
    "    if n_elem>601:\n",
    "        print(\"Fatal Error: il massimo dataset equilibrato è possibile con max 600 sample\")\n",
    "    \n",
    "    \n",
    "    F_x_eqWDE=[]\n",
    "    left2=[]\n",
    "    for ls in split_dataset:\n",
    "        ls1, left1 =random_list_range(ls,n_elem,randomize= randomize)\n",
    "        F_x_eqWDE.append(ls1)\n",
    "        left2.append(left1)\n",
    "    left_eqWDE=[]\n",
    "    for l2 in left2:#equilibre also the leftovers, i want to take the LAST n_lft element, so i made the left to be n_lft\n",
    "        tot_len=len(l2)\n",
    "        _, left3 =random_list_range(l2,tot_len-n_lft,randomize=randomize)\n",
    "        left_eqWDE.append(left3)\n",
    "    \n",
    "    F_x_eqWDE=np.array(F_x_eqWDE).reshape(-1,len(F_x_eqWDE[0][0]))\n",
    "    left_eqWDE=np.array(left_eqWDE).reshape(-1,len(left_eqWDE[0][0]))\n",
    "    \n",
    "    eq_mode_list=[]\n",
    "    left_mode_list=[]\n",
    "    for cls in classi:\n",
    "        tmp=[cls]*n_elem\n",
    "        eq_mode_list+=tmp\n",
    "        tmp=[cls]*n_lft\n",
    "        left_mode_list+=tmp\n",
    "    \n",
    "    eq_mode_list=np.array(eq_mode_list)\n",
    "    left_mode_list=np.array(left_mode_list)\n",
    "    \n",
    "    #print(f\"Abbiamo ottenuto {F_x_eqWDE.shape[0]} stride di {F_x_eqWDE.shape[1]} feature. {n_elem} stride per {len(classi)} label\")\n",
    "    \n",
    "    return F_x_eqWDE, eq_mode_list, left_eqWDE, left_mode_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a8599",
   "metadata": {},
   "source": [
    "\n",
    "def genStride_list(dataset_path, filename_list, mode: str=\"handheld\"):\n",
    "    \n",
    "    stop_list=[]\n",
    "    Stride_list=[]\n",
    "    mode_list=[]\n",
    "    filt_filename=[]\n",
    "    for filename in filename_list:\n",
    "        print(f\"--------Processing: {filename} --------\")\n",
    "        Stride_list1, mode_list1 =WDEselect(filename,dataset_path,mode)\n",
    "        Stride_list+=Stride_list1\n",
    "        mode_list+= mode_list1\n",
    "        if len(Stride_list1) != 0 :\n",
    "            stop_list.append(len(Stride_list))\n",
    "            filt_filename.append(filename)\n",
    "        \n",
    "    print(f\"\\nTotale: {len(Stride_list)} stride\")\n",
    "    return Stride_list, stop_list, filt_filename, mode_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44b1d34",
   "metadata": {},
   "source": [
    "\n",
    "def WDEselect(filename,dataset_path, mode: str=\"handheld\"):\n",
    "    '''\n",
    "    Select from WDE dataset only \"handheld\" mode\n",
    "    WDE dataset is segmented, every line is a json correspond to one stride\n",
    "    \n",
    "    in my PC dataset_path=\"C:/Users/aliba/OneDrive/Desktop/UNIVERSITA/TESI/DATASET/WalkingDistanceEstimation-master/dataset/\"\n",
    "    '''\n",
    "\n",
    "    #import all data from file\n",
    "    Stride_list=[]\n",
    "    mode_list=[]\n",
    "    stride={}\n",
    "    no_handheld=0\n",
    "    with open(dataset_path+filename+\".txt\",'r') as f:\n",
    "        for line in f:\n",
    "            stride=json.loads(line)\n",
    "            if (stride[\"mode\"] == mode) or (mode == \"all\"):\n",
    "                Stride_list.append(formatSLE(stride))\n",
    "                mode_list.append(stride[\"mode\"])\n",
    "            else:\n",
    "                no_handheld+=1\n",
    "\n",
    "\n",
    "    orig_len=len(Stride_list)\n",
    "    print(f\"Sono rimasti {orig_len} stride in mode {mode}; Sono stati eliminati {no_handheld} stride.\")\n",
    "    if orig_len != 0:\n",
    "        #eliminate outliers\n",
    "        Stride_list, mode_list =del_outliers(Stride_list,mode_list=mode_list,target_rng=[1,2],time_rng=[80,300])\n",
    "        print(\"I dati originari erano: \",orig_len,\", adesso sono:\",len(Stride_list),\"; Abbiamo eliminato \",orig_len-len(Stride_list),\" stride.\\n\")\n",
    "    return Stride_list, mode_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
